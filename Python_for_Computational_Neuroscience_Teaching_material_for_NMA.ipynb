{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Python for Computational Neuroscience - Teaching material for NMA ",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/helenxhou/nma2020python/blob/master/Python_for_Computational_Neuroscience_Teaching_material_for_NMA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7muY0mFx1xNY",
        "colab_type": "text"
      },
      "source": [
        "# Python Summary for Neuromatch Academy 2020\n",
        "\n",
        "__Content source:__ NMA tutorials ([github.com/NeuromatchAcademy/course-content/](https://github.com/NeuromatchAcademy/course-content/tree/master/tutorials))\n",
        "\n",
        "__Content compiler:__ pod019-evasive-seals (repo: [github.com/helenxhou](https://github.com/helenxhou/nma2020python))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7NTH4PZ2l49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# from matplotlib.colors import BoundaryNorm   # generate a colormap index\n",
        "# from mpl_toolkits.axes_grid1 import make_axes_locatable   # display multiple axes\n",
        "\n",
        "from scipy import stats\n",
        "from scipy.io import loadmat\n",
        "import scipy.optimize as opt\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "\n",
        "import sklearn as sk\n",
        "from sklearn import manifold, decomposition\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# from IPython.display import YouTubeVideo\n",
        "# !pip install plotly --quiet\n",
        "# import plotly.graph_objects as go\n",
        "# from plotly.colors import qualitative"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGYd3jdD2qdu",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "# @title Figure Settings\n",
        "import ipywidgets as widgets       # interactive display\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJKJonJs2347",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "# @title Helper functions\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUAcfCW_63QY",
        "colab_type": "text"
      },
      "source": [
        "# basics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1My2Y9Eq67Ne",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "type(spike_times)\n",
        "spike_times.shape\n",
        "Lt = range_t.size\n",
        "n_neurons = len(spike_times)\n",
        "\n",
        "help(restrict_spike_times)\n",
        "\n",
        "print(type([1,2,3])) # list\n",
        "print(type((1,2,3))) # tupple\n",
        "print(type(np.array([1,2,3]))) # numpy.ndarray\n",
        "print(type(np.array((1,2,3)))) # numpy.ndarray\n",
        "\n",
        "# print\n",
        "\n",
        "print(f'dG/drE(fp_lc) = {dGdrE_lc:.3f}')\n",
        "\n",
        "print(f\"Entropy for Neuron {neuron_idx}: {entropy(pmf):.2f} bits\")\n",
        "\n",
        "print(spike_times[0][range(0,6,2)])\n",
        "print(spike_times[0][slice(0,6,2)])\n",
        "\n",
        "print( \n",
        "  type(spike_times[idx]),\n",
        "  spike_times[idx].shape,\n",
        "  sep=\"\\n\",\n",
        ")\n",
        "\n",
        "# syntactic sweetners:\n",
        "\n",
        "ax_args['ylim'] = [ymin, ymax]\n",
        "ax.set(**ax_args)\n",
        "\n",
        "# loops\n",
        "\n",
        "for i in range(1, n_steps):\n",
        "\n",
        "for k in datasets.keys():\n",
        "\n",
        "for i, x_i in enumerate(xx):\n",
        "\n",
        "if isinstance(datasets, dict):\n",
        "  else:\n",
        "    if datasets is not None:\n",
        "      print('datasets argument should be a dict')\n",
        "      raise TypeError\n",
        "\n",
        "\n",
        "\n",
        "# function definition\n",
        "\n",
        "def my_perceived_motion(*args, **kwargs):\n",
        "  return [np.nan, np.nan]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vt-wwZd53F3o",
        "colab_type": "text"
      },
      "source": [
        "# NumPy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyO0xz9N3ce4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# slice/index arrays \n",
        "\n",
        "M = np.array(interval_spike_times, object)\n",
        "\n",
        "J = np.zeros((2, 2))\n",
        "K = np.ones((1,10))\n",
        "constant = np.ones_like(y)\n",
        "pselfmove_nomove = np.empty(thresholds.shape)\n",
        "pselfmove_nomove[:] = np.NaN\n",
        "np.isnan(x[samp_i + wind_i])\n",
        "\n",
        "x = np.arange(0, 10, .1)\n",
        "r = np.linspace(0, 1, 1000)\n",
        "experiment_duration = np.ptp(spike_times_flat)\n",
        "wm_vestibular = np.squeeze(vestibular[:, wm_idx])\n",
        "\n",
        "isis = np.diff(single_neuron_spikes)\n",
        "counts = np.insert(counts, 0, counts[0])\n",
        "mean_idx = np.searchsorted(bins, mean_isi)\n",
        "wm_idx = np.where(judgments[:, 0] == 0)\n",
        "rE = np.append(rE_init, np.zeros(Lt - 1))\n",
        "\n",
        "# stack arrays\n",
        "\n",
        "velpredict = np.concatenate((predictions[:, 3], predictions[:, 4]))\n",
        "design_matrix = np.hstack((design_matrix, x**degree)) # stack arrays in sequence horizontally (column wise)\n",
        "X = np.column_stack([constant, make_design_matrix(stim)]) # stack 1-D arrays as columns into a 2-D array\n",
        "\n",
        "\n",
        "# matrix operations\n",
        "\n",
        "A.transpose().flatten()\n",
        "theta_hat = (x.T @ y) / (x.T @ x)\n",
        "# theta_hat = np.matmul(x.T,y)/np.matmul(x.T,x)\n",
        "# theta_hat = np.dot(x,y)/np.dot(x,x)\n",
        "\n",
        "\n",
        "# calculation and stats\n",
        "\n",
        "h = -np.sum(pmf*np.log2(pmf))\n",
        "v = np.cumsum(a * dt)\n",
        "np.abs(h)\n",
        "pdf = 1 / np.sqrt(2 * np.pi * sigma**2) * np.exp(-(y - theta_hat * x)**2 / (2 * sigma**2))   # Gaussian likelihood  \n",
        "correlation = np.corrcoef(this_output_activity, current_activity)\n",
        "\n",
        "mean_spike_count = np.mean(total_spikes_per_neuron)\n",
        "median_spike_count = np.median(total_spikes_per_neuron)\n",
        "\n",
        "ymax = np.max(counts)\n",
        "ymax = np.maximum(y, y_hat) # element-wise maximum for two arrays\n",
        "xmin, xmax = np.floor(np.min(x)), np.ceil(np.max(x))\n",
        "\n",
        "counts, bins = np.histogram(isi, n_bins)\n",
        "\n",
        "# random number generation\n",
        "\n",
        "sample_idx = np.random.choice(len(x),size=len(x), replace=True)\n",
        " \n",
        "\n",
        "# linear algebra\n",
        "\n",
        "theta_hat = np.linalg.inv(X.T @ X) @ X.T @ y\n",
        "evals = np.linalg.eig(J)[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "obwd7KO13Oc2"
      },
      "source": [
        "# Matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UdKPmqv3dNl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(6, 4))\n",
        "\n",
        "# types of plots\n",
        "\n",
        "plt.plot(x, f, 'k')\n",
        "plt.hist(total_spikes_per_neuron, bins=50)\n",
        "plt.bar(x=[0, 1], height=[below, above])\n",
        "plt.scatter(veljudgmnt_self, velpredict_self, alpha=0.2)\n",
        "plt.eventplot(interval_spike_times[neuron_idx], color='.2')\n",
        "plt.stem(t, spikes[:nt], use_line_collection=True)\n",
        "plt.imshow(surface, origin='lower', aspect='auto', vmin=0, vmax=None, \n",
        "            cmap=plt.get_cmap('Wistia'), \n",
        "            extent=[xmin, xmax, ymin, ymax])\n",
        "\n",
        "# accessorize\n",
        "\n",
        "plt.axvline(mean_spike_count, color=\"orange\", label=\"Mean neuron\")\n",
        "plt.fill_between(bins, pmf_, step=\"pre\", alpha=0.4)\n",
        "plt.text(0.7, 0.8, f\"threshold:{thresholds[-idx]:0.2f}\\\n",
        "           \\ncorrect: {prop_correct[-idx]:0.2f}\")\n",
        "\n",
        "plt.xlabel('x (a.u.)', fontsize=14)\n",
        "plt.ylabel('F(x)', fontsize=14)\n",
        "plt.legend(loc=[1.01, 0.7],facecolor='xkcd:white')\n",
        "plt.xlim(-0.05, 1.01)\n",
        "plt.ylim(-0.05, 0.65)\n",
        "plt.xticks([0, 1], ['below', 'above'])\n",
        "plt.yticks([]);\n",
        "plt.setp(ax_x.spines.values(), visible=True)\n",
        "plt.colorbar(imx, cax=caxx)\n",
        "\n",
        "# subplots and axes\n",
        "\n",
        "fig, [ax1, ax2] = plt.subplots(nrows=1, ncols=2, sharex='col',\n",
        "                                 sharey='row', figsize=(14, 6))\n",
        "fig.suptitle('Sensory ground truth')\n",
        "ax1.set_title('world-motion condition')\n",
        "ax1.set_xlabel('observed')\n",
        "ax1.set_ylabel('predicted')\n",
        "ax1.set_xticklabels=['-20', '-10', '0']\n",
        "ax1.axhline(0, color=\".2\", linestyle=\"--\", zorder=1)\n",
        "ax1.set_zorder(1)\n",
        "# ax2 = plt.gca()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFUV2m9A71ze",
        "colab_type": "text"
      },
      "source": [
        "# SciPy.stats"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDUOLODC77IX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Poisson samples\n",
        "exc = stats.poisson(rate).rvs(n_steps)\n",
        "\n",
        "# exponential pdf\n",
        "pmf_exp = stats.expon.pdf(bins[1:], scale=mean_isi)\n",
        "\n",
        "# gamma pdf\n",
        "a = stats.gamma.pdf(np.arange(0, 10, dt), 2.5, 0)\n",
        "\n",
        "# linear least-squares regression\n",
        "p_value, std_err = stats.linregress(conditions, veljudgmnt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "da4WwL1T3Oqm"
      },
      "source": [
        "# SciPy.optimize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvDPdAKp74hP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# minimize example\n",
        "f = np.square\n",
        "res = opt.minimize(f, x0=2)\n",
        "print(\n",
        "  f\"Minimum value: {res['fun']:.4g}\",\n",
        "  f\"at x = {res['x']}\",\n",
        ")\n",
        "# minimize negative log likelihood\n",
        "x0 = np.random.normal(0, .2, 26)\n",
        "res = opt.minimize(neg_log_lik_lnp, x0, args=(X, y))\n",
        "\n",
        "def neg_log_lik_lnp(theta, X, y):\n",
        "  \"\"\"Return -loglike for the Poisson GLM model.\n",
        "\n",
        "  Args:\n",
        "    theta (1D array): Parameter vector.\n",
        "    X (2D array): Full design matrix.\n",
        "    y (1D array): Data values.\n",
        "\n",
        "  Returns:\n",
        "    number: Negative log likelihood.\n",
        "\n",
        "  \"\"\"\n",
        "  rate = np.exp(X @ theta)\n",
        "  log_lik = y @ np.log(rate) - rate.sum()\n",
        "  return -log_lik\n",
        "\n",
        "# root-finding algorithm\n",
        "x0 = np.array(0)\n",
        "x_fp = opt.root(my_WCr, x0).x.item()\n",
        "\n",
        "def my_WCr(x):\n",
        "  \"\"\"define the right hand of E dynamics\"\"\"\n",
        "  r = x\n",
        "  drdt = (-r + F(w * r + I_ext, a, theta))\n",
        "  y = np.array(drdt)\n",
        "  return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "56PA9PTjoStZ"
      },
      "source": [
        "# PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thy5yT0Oq3wm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build a neural network\n",
        "\n",
        "class DeepNet(nn.Module):\n",
        "  \"\"\"Deep network with one convolutional + max pooling input layer, and one \n",
        "  inear/fully connected output layer\n",
        "\n",
        "  Args:\n",
        "    h_in (int): height of input image, in pixels (i.e. number of rows)\n",
        "    w_in (int): width of input image, in pixels (i.e. number of columns)\n",
        "    C_in (int): number of input channels\n",
        "    C_out (int): number of conv kernels to convolve the input with\n",
        "    K (int): size of each conv kernel\n",
        "    Kpool (int): size of patches over which to pool \n",
        "\n",
        "  Attributes:\n",
        "    conv (nn.Conv2d): filter weights of convolutional layer\n",
        "    pool (nn.MaxPool2d): max pooling layer\n",
        "    dims (tuple of ints): dimensions of output from pool layer\n",
        "    fc (nn.Linear): weights and biases of fully connected layer\n",
        "    out (nn.Linear): weights and biases of output layer\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, h_in, w_in, C_in=1, C_out=8, K=9, Kpool=2):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Conv2d(c_in,c_out,kernel_size=K,padding=K//2, stride=1)\n",
        "    self.pool = nn.MaxPool2d(Kpool)\n",
        "    self.dims = (C_out, h_in // Kpool, w_in // Kpool)  # dimensions of pool layer output\n",
        "    M = np.prod(self.dims) # number of hidden units\n",
        "    self.fc = nn.Linear(M,10) # flattened pool output --> 10D representation\n",
        "    self.out_layer = nn.Linear(10,1) # 10D representation --> scalar\n",
        "\n",
        "    nn.init.normal_(self.out_layer.weight, std=0.01) # initialize weights to be small\n",
        "\n",
        "  def forward(self,x):\n",
        "    \"\"\"Deep network with one convolutional layer followed by a max pooling layer\n",
        "    and one fully connected layer\n",
        "\n",
        "    Args:\n",
        "      x (torch.Tensor): p x 48 x 64 tensor with pixel grayscale values for each \n",
        "      of p stimulus images.\n",
        "\n",
        "    Returns:\n",
        "      y (torch.Tensor): p x 1 tensor output\n",
        "\n",
        "  \"\"\"\n",
        "    x = x.unsqueeze(1) # p x 1 x h, add a singleton dimention\n",
        "    h = torch.relu(self.conv(x)) # non-linear/rectified activation function\n",
        "    h = self.pool(h)\n",
        "    h = h.view(-1, np.prod(self.dims))  # flatten each convolutional + pooling layer output into a vector\n",
        "    h = torch.relu(self.fc(h))  # output of fully connected layer\n",
        "    y = torch.sigmoid(self.out_layer(h))  # network output -> probability b/t 0 and 1\n",
        "\n",
        "    return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y27QwCj_QUAO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# example: build an autoencoder with sequential container\n",
        "\n",
        "model = nn.Sequential(nn.Linear(input_size, encoding_size),\n",
        "                      nn.PReLU(), # bottleneck layer: parametric ReLU\n",
        "                      NormalizeLayer()\n",
        "                      nn.Linear(encoding_size, input_size),\n",
        "                      nn.Sigmoid())\n",
        "\n",
        "class NormalizeLayer(nn.Module):\n",
        "  \"\"\"\n",
        "  pyTorch layer that normalizes activations by their L2 norm.\n",
        "\n",
        "  Args:\n",
        "      None.\n",
        "            \n",
        "  Returns:\n",
        "      Object inherited from nn.Module class.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "      \n",
        "  def forward(self, x):\n",
        "    return nn.functional.normalize(x, p=2, dim=1)\n",
        "\n",
        "  \n",
        "# after training:\n",
        "with torch.no_grad():\n",
        "  output_test = model(input_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "chdVbmTZoStf",
        "colab": {}
      },
      "source": [
        "# train a network\n",
        "\n",
        "def train(net, loss_fn, train_data, train_labels, n_iter=50, learning_rate=1e-4,momentum=.9):\n",
        "  \"\"\"Run gradient descent to opimize parameters of a given network\n",
        "\n",
        "  Args:\n",
        "    net (nn.Module): PyTorch network whose parameters to optimize\n",
        "    loss_fn: built-in PyTorch loss function to minimize\n",
        "    train_data (torch.Tensor): n_train x n_neurons tensor with neural\n",
        "      responses to train on\n",
        "    train_labels (torch.Tensor): n_train x 1 tensor with orientations of the\n",
        "      stimuli corresponding to each row of train_data, in radians\n",
        "    n_iter (int): number of iterations of gradient descent to run\n",
        "    learning_rate (float): learning rate to use for gradient descent\n",
        "\n",
        "  \"\"\"\n",
        "  # Initialize MSE loss function\n",
        "  loss_fn = nn.MSELoss()\n",
        "  # loss_fn = nn.BCELoss() # alternative:binary cross-entropy\n",
        "  # # add regularization to loss\n",
        "  # L2 = L2_penalty * torch.square(weights).sum()\n",
        "  # L1 = L1_penalty * torch.abs(weights).sum()\n",
        "  # loss = loss_fn(output, target) + L1 + L2\n",
        "\n",
        "  # Initialize SGD optimizer with momentum: try vary mini-batch size\n",
        "  optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)\n",
        "  # optimizer = optim.Adam(model.parameters()) # alternative: Adam\n",
        "\n",
        "  # Placeholder to save the loss at each iteration\n",
        "  track_loss = []\n",
        "\n",
        "  # Loop over epochs (cf. appendix)\n",
        "  for i in range(n_iter):\n",
        "\n",
        "    # Evaluate loss using loss_fn\n",
        "    out = net(train_data)  # compute network output from inputs in train_data\n",
        "    loss = loss_fn(out, train_labels)  # evaluate loss function\n",
        "\n",
        "    # Compute gradients\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Update weights\n",
        "    optimizer.step()\n",
        "\n",
        "    # Store current value of loss\n",
        "    track_loss.append(loss.item())  # .item() needed to transform the tensor output of loss_fn to a scalar\n",
        "\n",
        "    # Track progress\n",
        "    if (i + 1) % (n_iter // 5) == 0:\n",
        "      print(f'iteration {i + 1}/{n_iter} | loss: {loss.item():.3f}')\n",
        "\n",
        "  print('training done!')\n",
        "  return track_loss\n",
        "  \n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# Initialize network\n",
        "net = DeepNet(n_neurons, 20)\n",
        "\n",
        "# Run GD on data\n",
        "train_loss = train(net, loss_fn, resp_train, stimuli_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4ZvARL8loVQ7"
      },
      "source": [
        "# Scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aF4DAH8-oVQ9",
        "colab": {}
      },
      "source": [
        "# Dimentionality reduction:\n",
        "\n",
        "# linear methods: e.g. PCA\n",
        "# (see also : probabilistic PCA, factor acanlysis, supervised: linear discriminant analysis/LDA)\n",
        "pca_model = decomposition.PCA(n_components=10) # Initializes PCA\n",
        "pca_model.fit(X) # Performs PCA\n",
        "scores = pca_model.transform(X) # Obtain latent space representation\n",
        "pca_components = pca_model.components_\n",
        "pca_output = pca.inverse_transform(scores)\n",
        "# resp_lowd = decomposition.PCA(n_components=min(20, resp.shape[1])).fit_transform(resp)\n",
        "\n",
        "# nonlinear methods: e.g t-SNE (t-distributed stochastic neighbor embedding)\n",
        "# manifold --> embedding\n",
        "sne_model = manifold.TSNE(n_components=2, perplexity=30, random_state=2020) # Initializes t-SNE\n",
        "embed = tsne_model.fit_transform(X) # Performs t-SNE\n",
        "plt.scatter(embed[:, 0], embed[:, 1], labels)\n",
        "\n",
        "# NMF (non-negative matrix factorization)\n",
        "nmf_model = decomposition.NMF(n_components=2, init='random')\n",
        "nmf.fit(input_train+1.)\n",
        "nmf_latent_test = nmf.transform(input_test+1.)\n",
        "nmf_components = nmf.components_\n",
        "nmf_output_test = nmf.inverse_transform(nmf_latent_test)\n",
        "\n",
        "# truncated SVD\n",
        "svd = decomposition.TruncatedSVD(n_components=2)\n",
        "svd.fit(input_train)\n",
        "svd_latent_train = svd.transform(input_train)\n",
        "svd_latent_test = svd.transform(input_test)\n",
        "svd_reconstruction_train = svd.inverse_transform(svd_latent_train)\n",
        "svd_reconstruction_test = svd.inverse_transform(svd_latent_test)\n",
        "\n",
        "# fetch data from openml\n",
        "mnist = fetch_openml(name='mnist_784')\n",
        "X = mnist.data\n",
        "labels = [int(k) for k in mnist.target]\n",
        "labels = np.array(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n_eJ967RTd8X"
      },
      "source": [
        "# BONUS: from team project **Seals Evading Hidden States**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sr0Yv7cjTd8a",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "#!pip install factor_analyzer\n",
        "!pip install hmmlearn\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "#from factor_analyzer import FactorAnalyzer\n",
        "\n",
        "# from HMM tutorial\n",
        "import scipy.stats as ss\n",
        "import pandas as pd\n",
        "from hmmlearn import hmm\n",
        "from matplotlib import patches\n",
        "\n",
        "# Simulated data\n",
        "import random\n",
        "import seaborn as sns\n",
        "\n",
        "# figure settings\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMyR3kX5SxFH",
        "colab_type": "text"
      },
      "source": [
        "Visualization Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SBtUXuDSTd8c",
        "colab": {}
      },
      "source": [
        "# code for firing rates\n",
        "def plot_firing_rates(time, fr, linewidth=None):\n",
        "  \"\"\"Plots firing rates over specified time range.\n",
        "\n",
        "  Args:\n",
        "    time (numpy array of float):          Starts of timebins.\n",
        "    fr (numpy array of floats):           Firing rates in each timebin.\n",
        "    linewidth (float):                    Linewidth (optional).\n",
        "  \"\"\"\n",
        "\n",
        "  sns.set()\n",
        "  sns.set_style('whitegrid')\n",
        "  sns.set_style('white')\n",
        "  sns.set_style(\"ticks\")\n",
        "  sns.set_context(\"talk\", font_scale=1.9, rc={\"lines.linewidth\":3.3}) \n",
        "  if len(np.nonzero(linewidth)) == 0:\n",
        "    plt.plot(time, fr)\n",
        "  elif len(np.nonzero(linewidth)) != 0:\n",
        "    plt.plot(time, fr, linewidth=linewidth)\n",
        "  sns.despine()\n",
        "  plt.ylabel('rate (Hz)')\n",
        "  plt.xlabel('time (ms)')\n",
        "  return\n",
        "  \n",
        "# For spike rasters\n",
        "def plot_raster_function(spike_times, spike_indices, markersize):\n",
        "  \"\"\"Plot raster of the whole experiment.\n",
        "\n",
        "  Args:\n",
        "    spike_times (numpy array of float):   Spike times (ms).\n",
        "    spike_indices (numpy array of ints):  Array index of each spike.\n",
        "    markersize (float):                   Marker size.\n",
        "  \"\"\"\n",
        "  # Input spike times in ms, but we plot them in seconds.\n",
        "  sns.set()\n",
        "  sns.set_style('whitegrid')\n",
        "  sns.set_style('white')\n",
        "  sns.set_style(\"ticks\")\n",
        "  sns.set_context(\"talk\", font_scale=1.9, rc={\"lines.linewidth\":3.3}) \n",
        "  plt.plot(spike_times/1000,spike_indices+1,'k.',markersize=markersize)\n",
        "  plt.xlim((0,np.max(spike_times)/1000))\n",
        "  plt.ylim((0,np.max(spike_indices)))\n",
        "  plt.yticks((1,np.max(spike_indices)+1))\n",
        "  plt.xticks((0,np.max(spike_times)/1000))\n",
        "  plt.xlabel('Time (s)')\n",
        "  plt.ylabel('Neuron')\n",
        "\n",
        "# For dimensionality reduction\n",
        "# From tutorials\n",
        "def plot_variance_explained(variance_explained):\n",
        "  \"\"\"Plots eigenvalues.\n",
        "\n",
        "  Args:\n",
        "    variance_explained (numpy array of floats) : Vector of variance explained\n",
        "                                                 for each PC\n",
        "  \"\"\"\n",
        "  fig=plt.figure(figsize=(8,5))\n",
        "  plt.plot(np.arange(1, len(variance_explained) + 1), variance_explained,\n",
        "           '--k')\n",
        "  plt.xlabel('Number of components')\n",
        "  plt.ylabel('Variance explained')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_eigenvalues(evals):\n",
        "  \"\"\"Plots eigenvalues.\n",
        "\n",
        "  Args:\n",
        "     (numpy array of floats) : Vector of eigenvalues\n",
        "  \"\"\"\n",
        "  fig=plt.figure(figsize=(8,5))\n",
        "  plt.plot(np.arange(1, len(evals) + 1), evals, 'o-k')\n",
        "  plt.xlabel('Component')\n",
        "  plt.ylabel('Eigenvalue')\n",
        "  plt.title('Scree plot')\n",
        "  plt.show()\n",
        "\n",
        "#For HMM\n",
        "def plot_states(real_states, states, obs_label, title):\n",
        "  \"\"\"Plots HMM states and observations for 1d states and observations.\n",
        "\n",
        "  Args:\n",
        "    real_states (numpy array of floats):  Observed or simulated states.\n",
        "    states (numpy array of floats):       HMM latent states.\n",
        "    obs_label (string):                   Observed/simulated state label.\n",
        "    title (string):                       Plot title.\n",
        "  \"\"\"\n",
        "  plt.plot(real_states, label=obs_label, c=\"blue\")\n",
        "  plt.plot(states, \"--\", label=\"Latent States\", c=\"orange\")\n",
        "  plt.yticks(ticks=[0,1], labels=['Incorrect','Correct'])\n",
        "  plt.xlabel(\"Trials\")\n",
        "  plt.ylabel(\"State\")\n",
        "  plt.title(\"Simulated correct/incorrect trials\")\n",
        "  leg = plt.legend(bbox_to_anchor=(1.04,1), loc=\"upper left\",frameon='none',markerscale=1)\n",
        "  leg.get_frame().set_linewidth(0.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WR0-fhVzS2MI",
        "colab_type": "text"
      },
      "source": [
        "Data Processing Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiQrlfehS344",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert [neuron_id, spiketime] format to neurons x spike count format\n",
        "def bin_spikes(neuron_ids, spike_train, end_time, bin):\n",
        "  \"\"\"Converts spike timestamps into binned spike counts & smoothed firing rates\n",
        "        \n",
        "      Args:\n",
        "        neuron_ids (numpy array of ints):     ID of the neuron of each spike.\n",
        "        spike_train (numpy array of floats):  Spike timestamps.\n",
        "        end_time (float):                     End timestamp of spike traces.\n",
        "        bin (int):                            Bin size (ms).\n",
        "\n",
        "      Returns:\n",
        "        spike_counts (numpy array of ints):   Spike counts per bin (neurons, spikes).\n",
        "        firing_rates (numpy array of floats): Firing rates in each bin (neurons, rates).\n",
        "    \"\"\"\n",
        "  Num_neurons = (np.max(neuron_ids) + 1).astype(int)\n",
        "  timeVector = np.arange(0,end_time,bin)\n",
        "  spike_counts = np.zeros((Num_neurons,len(timeVector)-1))\n",
        "\n",
        "  # Counts spikes in bins of [bin] ms.\n",
        "  for i in range(Num_neurons):\n",
        "    hist, bin_edges = np.histogram(spike_train[neuron_ids==i],bins = timeVector) \n",
        "    spike_counts[i,:] = hist \n",
        "  RateT = spike_counts / bin *1000 #convert to Hz\n",
        "\n",
        "  # Slide a moving average window over the rates to smooth.\n",
        "  Num_points = np.int(len(RateT[0]))\n",
        "  firing_rates = np.zeros((Num_neurons,Num_points))\n",
        "  # Smooth the rate of each neuron separately.\n",
        "  for j in range(Num_neurons):  \n",
        "    firing_rates[j,:] = np.convolve(RateT[j], np.ones((window,))/window, mode='same')\n",
        "\n",
        "  return spike_counts, firing_rates\n",
        "\n",
        "# Spike traces for HMM\n",
        "def trace_to_trials_function(firing_rate, TrialTime, dtRate, Num_trials):\n",
        "  \"\"\"Transform the trace of the whole experiment into a matrix with rows=samples, cols=trials\n",
        "\n",
        "  Args:\n",
        "    firing_rate (numpy array of float):   Firing rates over entire trace.\n",
        "    TrialTime (float):                    Length of trial (ms).\n",
        "    dtRate (int):                         Bin size (ms).\n",
        "    Num_trials (int):                     # of trials.\n",
        "\n",
        "  Returns:\n",
        "    fr_trials (numpy array of float):     Firing rates parcellated by trial.\n",
        "  \"\"\"\n",
        "\n",
        "  # Divide trial time by bin size to get number of samples in a trial.\n",
        "  fr_trials = np.zeros((TrialTime//dtRate,Num_trials)) \n",
        "  for trial in range(Num_trials):\n",
        "      trial_start = TrialTime//dtRate * trial\n",
        "      trial_end = TrialTime//dtRate * (trial+1)\n",
        "      fr_trials[:,trial] = firing_rate[trial_start:trial_end]\n",
        "  return fr_trials\n",
        "\n",
        "# HMM accuracy\n",
        "def get_accuracy(real_states, states):\n",
        "  \"\"\"Matches state assignments & calculates percent accuracy\n",
        "\n",
        "  Args:\n",
        "    real_states (numpy array of floats):  Observed or simulated states.\n",
        "    states (numpy array of floats):       HMM latent states.\n",
        "\n",
        "  Returns:\n",
        "    per_correct (float):                  % accuracy of classification.\n",
        "    states (numpy array of floats):       States, inverted if necessary.\n",
        "  \"\"\"\n",
        "  per_correct = (1 - np.sum(np.abs(real_states-states))/len(states))*100\n",
        "  if per_correct < 50:\n",
        "    per_correct = 100-per_correct\n",
        "    states = 1-states\n",
        "  return per_correct, states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUOtM57pS8uh",
        "colab_type": "text"
      },
      "source": [
        "Define functions for simulation of Poisson network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEPPQIKTS82Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Simulate uncorrelated data for testing our HMM with low and high firing rates.\n",
        "def simulate_trials_different_rates(Num_neurons, Num_trials, Fraction_of_correct_trials, \n",
        "                    rate_baseline, rate_elevated, TrialTime, dtRate, window):\n",
        "  \"\"\"Transform the trace of the whole experiment into a matrix with rows=samples, cols=trials\n",
        "\n",
        "  Args:\n",
        "    Num_neurons (int):                    # neurons to simulate\n",
        "    Num_trials (int):                     # trials\n",
        "    Fraction_of_correct_trials (float):   fraction incorrect (between 0 and 1)\n",
        "    rate_baseline (float):                firing rate (Hz)\n",
        "    rate_elevated (float):                firing rate during correct trials (Hz)\n",
        "    TrialTime (int):                      average trial length (ms)\n",
        "    dtRate (int):                         firing rate bin (ms)\n",
        "    window (int):                         smoothing window (ms)\n",
        "\n",
        "  Returns:\n",
        "    spike_train (numpy array of floats):  Poisson spike train across all time,\n",
        "                                            size (neurons, time in ms)\n",
        "  \"\"\"\n",
        "\n",
        "  Num_correct_trials = int(np.floor(Fraction_of_correct_trials * Num_trials))\n",
        "  # Generate Poisson spike trains if neurons are uncorrelated.\n",
        "  # Randomly interleave correct and incorrect trials.\n",
        "  trials_correct = np.sort(np.random.choice(Num_trials, size=Num_correct_trials, replace=False))\n",
        "  trials_wrong = np.setdiff1d(np.arange(0,Num_trials,step=1),trials_correct)\n",
        "\n",
        "  # Preallocate memory for the spike train of the whole experiment.\n",
        "  spike_train = np.zeros((int(Num_neurons*rate_elevated*TrialTime*Num_trials*2),2))\n",
        "  temp = 0 # This is just use to facilitate saving of spikes given that they vary in number at each trial.\n",
        "  # Loop over trials and generate uncorrelated spikes for each neuron in each trial.\n",
        "  for trial in range(Num_trials):\n",
        "    # If the trial is correct.\n",
        "    if trial in trials_correct: # Do correct and wrong trials separately.\n",
        "      time_start_trial = TrialTime*trial\n",
        "      time_end_trial = TrialTime*(trial+1)\n",
        "      if trial == 0:\n",
        "        temp = 0\n",
        "      else:\n",
        "        temp += len(sx[0]) # Use previous one to know where to save spikes.\n",
        "      # Generate a random number of spikes for all neurons over the trial.\n",
        "      Num_spike=np.random.poisson(Num_neurons*rate_elevated*TrialTime)\n",
        "      # Spread the spikes over neurons and time.\n",
        "      st=np.random.uniform(low=time_start_trial,high=time_end_trial,size=(Num_spike,1))\n",
        "      sx=np.zeros((2,len(st))) \n",
        "      sx[0]=np.sort(st)[:,0] # Spike times\n",
        "      sx[1]=np.random.choice(Num_neurons,size=len(st)) # Neuron indices\n",
        "      # Save the spike train\n",
        "      spike_train[temp:temp+len(sx[0]),0] = sx[0] # Spike times\n",
        "      spike_train[temp:temp+len(sx[1]),1] = sx[1] # Neuron indices\n",
        "    else: # If the trial is incorrect.\n",
        "      time_start_trial = TrialTime*trial\n",
        "      time_end_trial = TrialTime*(trial+1)\n",
        "      if trial == 0:\n",
        "        temp = 0\n",
        "      else:\n",
        "        temp += len(sx[0]) # Use previous one to know where to save spikes.\n",
        "      # Generate a random number of total spikes.\n",
        "      Num_spike=np.random.poisson(Num_neurons*rate_baseline*TrialTime)\n",
        "      # Spread them over time.\n",
        "      st=np.random.uniform(low=time_start_trial,high=time_end_trial,size=(Num_spike,1))\n",
        "      sx=np.zeros((2,len(st)))\n",
        "      sx[0]=np.sort(st)[:,0] # Sort by time.\n",
        "      sx[1]=np.random.choice(Num_neurons,size=len(st)) # neuron indices\n",
        "      # Save the spike train\n",
        "      spike_train[temp:temp+len(sx[0]),0] = sx[0] # Spike times\n",
        "      spike_train[temp:temp+len(sx[1]),1] = sx[1] # Neuron indices\n",
        "  # Get rid of padded zeros (because we didn't know a priori what the total # of spikes was going to be).\n",
        "  spike_train = spike_train[np.where(spike_train[:,0]!=0)]\n",
        "\n",
        "  spike_counts, firing_rates = bin_spikes(spike_train[:,1], spike_train[:,0], \n",
        "                                          TrialTime*Num_trials+dtRate, dtRate)\n",
        "  \n",
        "  return spike_train, firing_rates, trials_correct, spike_counts\n",
        "\n",
        "# This code block generates correlated Poisson spike trains.\n",
        "# In correct trials, neurons are more correlated. While in wrong trials neurons have low correlations.\n",
        "def simulate_trials_different_corrs(Num_neurons, Num_trials, Fraction_of_correct_trials, rate_baseline, \n",
        "                    taujitter,c_baseline, c_elevated, TrialTime, dtRate, window):\n",
        "  \"\"\"Transform the trace of the whole experiment into a matrix with rows=samples, cols=trials\n",
        "\n",
        "  Args:\n",
        "    Num_neurons (int):                    # neurons to simulate\n",
        "    Num_trials (int):                     # trials\n",
        "    Fraction_of_correct_trials (float):   fraction incorrect (between 0 and 1)\n",
        "    rate_baseline (float):                firing rate (Hz) - constant across trials\n",
        "    taujitter (float):                    Mean jittering of spike timing (ms). This is used to avoid perfect synchrony.\n",
        "    c_baseline (float):                   pairwise correlations during incorrect trials (dimensionless)\n",
        "    c_elevated (float):                   pairwise correlations during correct trials (dimensionless)\n",
        "    TrialTime (int):                      average trial length (ms)\n",
        "    dtRate (int):                         firing rate bin (ms)\n",
        "    window (int):                         smoothing window (ms)\n",
        "\n",
        "  Returns:\n",
        "    spike_train (numpy array of floats):  Poisson spike train across all time,\n",
        "                                            size (neurons, time in ms)\n",
        "  \"\"\"\n",
        "\n",
        "  Num_correct_trials = int(np.floor(Fraction_of_correct_trials * Num_trials))\n",
        "  # Generate Poisson spike trains if neurons are uncorrelated.\n",
        "  # Randomly interleave correct and incorrect trials.\n",
        "  trials_correct = np.sort(np.random.choice(Num_trials, size=Num_correct_trials, replace=False))\n",
        "  trials_wrong = np.setdiff1d(np.arange(0,Num_trials,step=1),trials_correct)\n",
        "  # Preallocate memory for the spike train of the whole experiment.\n",
        "  spike_train = np.zeros((int(Num_neurons*rate_elevated*TrialTime*Num_trials*2),2))\n",
        "  temp = 0 # This is just use to facilitate saving of spikes given that they vary in number at each trial.\n",
        "  # Loop over trials to generate spike trains for all neurons in each trial.\n",
        "  for trial in range(Num_trials):\n",
        "    if trial in trials_correct: # If trial is correct.\n",
        "      rm=rate_baseline/c_elevated # Firing rate of mother process.\n",
        "      # Define the beginning and end of trial.\n",
        "      time_start_trial = TrialTime*trial\n",
        "      time_end_trial = TrialTime*(trial+1)\n",
        "      if trial == 0:\n",
        "        temp = 0\n",
        "      else:\n",
        "        temp += len(sx[0]) # Use previous one to know where to save spikes.\n",
        "      # Define a mother process to generate correlated Poisson spikes.\n",
        "      Num_spike_moth=np.random.poisson(rm*TrialTime); # Number of mother spikes\n",
        "      stm=np.random.uniform(low=time_start_trial,high=time_end_trial,size=(Num_spike_moth,1))\n",
        "      maxnsx=int(TrialTime*rate_baseline*Num_neurons*2); # Max num spikes\n",
        "      sx=np.zeros((2,maxnsx)) \n",
        "      ns=0\n",
        "      # Loop over each neuron to generate its spike train in each trial.\n",
        "      for j in range(Num_neurons):\n",
        "        ns0 = np.random.binomial(Num_spike_moth,c_elevated)\n",
        "        st=random.sample(list(stm[:,0]),ns0) # Sample spike times randomly\n",
        "        st=st+taujitter*np.random.normal(0,1,size=len(st)) # jitter spike times\n",
        "        st=st[(st>time_start_trial) & (st<time_end_trial)] # Get rid of out-of-bounds times\n",
        "        ns0=len(st) # Re-compute spike count\n",
        "        sx[0,ns:ns+ns0]=st # Set the spike times and indices        \n",
        "        sx[1,ns:ns+ns0]=j\n",
        "        ns=ns+ns0\n",
        "      # Get rid of padded zeros, since we didn't initially know how many spikes there would be.\n",
        "      sx = sx[:,sx[0,:]>0]\n",
        "      # Sort by spike time\n",
        "      I = np.argsort(sx[0,:])\n",
        "      sx = sx[:,I]\n",
        "      nspikeX=len(sx[0,:])\n",
        "      # Save spike times and indices.\n",
        "      spike_train[temp:temp+len(sx[0]),0] = sx[0] # Spike times\n",
        "      spike_train[temp:temp+len(sx[1]),1] = sx[1] # Neuron indices\n",
        "    else:\n",
        "      # if trial is wrong.\n",
        "      rm=rate_baseline/c_baseline # Firing rate of mother process\n",
        "      # Define the beginning and end of trial.\n",
        "      time_start_trial = TrialTime*trial\n",
        "      time_end_trial = TrialTime*(trial+1)\n",
        "      if trial == 0:\n",
        "        temp = 0\n",
        "      else:\n",
        "        temp += len(sx[0]) # Use previous one to know where to save spikes.\n",
        "      # If correct trial.\n",
        "      # If correlated\n",
        "      Num_spike_moth=np.random.poisson(rm*TrialTime); # Number of mother spikes\n",
        "      stm=np.random.uniform(low=time_start_trial,high=time_end_trial,size=(Num_spike_moth,1))\n",
        "      maxnsx=int(TrialTime*rate_baseline*Num_neurons*2) # Max num spikes\n",
        "      sx=np.zeros((2,maxnsx)) \n",
        "      ns=0\n",
        "      # Loop over each neuron to generate its spike train in each trial.\n",
        "      for j in range(Num_neurons):\n",
        "        ns0 = np.random.binomial(Num_spike_moth,c_baseline)\n",
        "        st=random.sample(list(stm[:,0]),ns0) # Sample spike times randomly\n",
        "        st=st+taujitter*np.random.normal(0,1,size=len(st)) # jitter spike times\n",
        "        st=st[(st>time_start_trial) & (st<time_end_trial)] # Get rid of out-of-bounds times\n",
        "        ns0=len(st) # Re-compute spike count\n",
        "        sx[0,ns:ns+ns0]=st # Set the spike times        \n",
        "        sx[1,ns:ns+ns0]=j # Neuron index\n",
        "        ns=ns+ns0\n",
        "      # Get rid of padded zeros\n",
        "      sx = sx[:,sx[0,:]>0]\n",
        "      # Sort by spike time\n",
        "      I = np.argsort(sx[0,:])\n",
        "      sx = sx[:,I]\n",
        "      nspikeX=len(sx[0,:])\n",
        "      # Save spike times and indices.\n",
        "      spike_train[temp:temp+len(sx[0]),0] = sx[0] # Spike times\n",
        "      spike_train[temp:temp+len(sx[1]),1] = sx[1] # Neuron indices\n",
        "  # Get rid of padded zeros.\n",
        "  spike_train = spike_train[np.where(spike_train[:,0]!=0)]\n",
        "  # Count spikes in bins of size dtRate milliseconds.\n",
        "  timeVector = np.arange(0,TrialTime*Num_trials+dtRate,dtRate) \n",
        "  spike_counts = np.zeros((Num_neurons,len(timeVector)-1))\n",
        "  for i in range(Num_neurons):\n",
        "    hist, bin_edges = np.histogram(spike_train[spike_train[:,1]==i,0]\n",
        "                    ,bins = timeVector) \n",
        "    spike_counts[i,:] = hist \n",
        "  RateT = spike_counts / dtRate *1000 # Multiply by 1000 to turn kHz into Hz.\n",
        "  # Slide a moving average window over the rates to smooth them more if necessary.\n",
        "  Num_points = np.int(len(RateT[0]))\n",
        "  firing_rates = np.zeros((Num_neurons,Num_points))\n",
        "  # Smooth the rate of each neuron separately.\n",
        "  for j in range(Num_neurons):  \n",
        "    firing_rates[j,:] = np.convolve(RateT[j], np.ones((window,))/window, mode='same')\n",
        "  return spike_train, firing_rates, trials_correct, spike_counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E9NCq5nGQHc8"
      },
      "source": [
        "# BONUS: from team project **The hybrid seals AKA The chimeric seals**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "saEXUtV9QQLe",
        "colab": {}
      },
      "source": [
        "# import"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FOPcfNt1QQpE",
        "colab": {}
      },
      "source": [
        "# code"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}